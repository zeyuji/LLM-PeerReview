<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Scoring, Reasoning, and Selecting the Best! Ensembling Large Language Models via a Peer-Review Process">
  <meta name="title" content="Scoring, Reasoning, and Selecting the Best! Ensembling Large Language Models via a Peer-Review Process">
  <meta name="keywords" content="Ensemble Learning, LLM Ensemble, Peer-Review Process">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:title" content="Scoring, Reasoning, and Selecting the Best! Ensembling Large Language Models via a Peer-Review Process"/>
  <meta property="og:description" content="LLM-PeerReview: An unsupervised LLM Ensemble method that selects the most ideal response from multiple LLM-generated candidates for each query, harnessing the collective wisdom of multiple models with diverse strengths."/>
  <meta property="og:url" content="https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/logo.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <!-- Twitter -->
  <meta name="twitter:title" content="Scoring, Reasoning, and Selecting the Best! Ensembling Large Language Models via a Peer-Review Process">
  <meta name="twitter:description" content="LLM-PeerReview: An unsupervised LLM Ensemble method that selects the most ideal response from multiple LLM-generated candidates for each query, harnessing the collective wisdom of multiple models with diverse strengths.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/logo.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->


  <title>Scoring, Reasoning, and Selecting the Best! Ensembling Large Language Models via a Peer-Review Process</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <!-- MathJax for rendering mathematical formulas -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
    <style>
      /* Tab content styles */
      .tab-pane {
        display: none;
        opacity: 0;
        transition: opacity 0.3s ease;
      }
      .tab-pane.is-active {
        display: block;
        opacity: 1;
      }
      
      /* Make sure the notification boxes have consistent height */
      .notification {
        min-height: 200px;
        overflow-y: auto;
        max-height: 500px;
        white-space: pre-wrap;
        font-family: monospace;
        line-height: 1.5;
        font-size: 0.9em;
      }
      
      /* Loading indicator */
      .loading {
        text-align: center;
        padding: 20px;
        font-style: italic;
        color: #888;
      }
      
      /* Math formula styles */
      .mjx-chtml {
        display: inline-block;
        margin: 2px 0;
      }
      
      /* Ensure inline math doesn't break line height */
      .mjx-chtml.MJXc-display {
        margin: 1em 0;
        padding: 0.5em 0;
        overflow-x: auto;
        overflow-y: hidden;
      }
      
      /* Improve readability of math in dark notification boxes */
      .notification .mjx-chtml {
        color: #333;
        background-color: rgba(255, 255, 255, 0.9);
        padding: 2px 4px;
        border-radius: 3px;
      }

      .paragraph-title {
        font-weight: bold;
        margin-right: 0.5em;
      }

      .center {
        display: block;
        margin-left: auto;
        margin-right: auto;
      }
    </style>
</head>
<body>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            Scoring, Reasoning, and Selecting the Best!<br>
            Ensembling Large Language Models via a Peer-Review Process</h1>
          <div class="is-size-5 publication-authors">
            <!-- Paper authors -->
            <!-- <span class="author-block">
                <a href="LINK" target="_blank">Zhijun Chen</a><sup>*</sup>,
            </span>
            <span class="author-block">
                <a href="LINK" target="_blank">Zeyu Ji</a><sup>*</sup>,
            </span>
            <span class="author-block">
                <a href="LINK" target="_blank">Qianren Mao</a>,
            </span>
            <span class="author-block">
                <a href="LINK" target="_blank">Junhang Cheng</a>,
            </span>
            <span class="author-block">
                <a href="LINK" target="_blank">Bangjie Qin</a>,
            </span>
            <span class="author-block">
                <a href="LINK" target="_blank">Hao Wu</a>,
            </span>
            <span class="author-block">
                <a href="LINK" target="_blank">Zhuorran Li</a>,
            </span>
            <span class="author-block">
                <a href="LINK" target="_blank">Jingzheng Li</a>,
            </span>
            <span class="author-block">
                <a href="LINK" target="_blank">Kai Sun</a>,
            </span>
            <span class="author-block">
                <a href="LINK" target="_blank">Zizhe Wang</a>,
            </span>
            <span class="author-block">
                <a href="LINK" target="_blank">Zhu Sun</a>,
            </span>
            <span class="author-block">
                <a href="LINK" target="_blank">Xiangyang Ji</a>,
            </span>
            <span class="author-block">
                <a href="LINK" target="_blank">Hailong Sun</a><sup>†</sup>
            </span> -->
            <span class="author-block">Zhijun Chen<sup>*</sup>,</span>
            <span class="author-block">Zeyu Ji<sup>*</sup>,</span>
            <span class="author-block">Qianren Mao,</span>
            <span class="author-block">Junhang Cheng,</span>
            <span class="author-block">Bangjie Qin,</span>
            <span class="author-block">Hao Wu,</span>
            <span class="author-block">Zhuorran Li,</span>
            <span class="author-block">Jingzheng Li,</span>
            <span class="author-block">Kai Sun,</span>
            <span class="author-block">Zizhe Wang,</span>
            <span class="author-block">Yikun Ban,</span>
            <span class="author-block">Zhu Sun,</span>
            <span class="author-block">Xiangyang Ji,</span>
            <span class="author-block">Hailong Sun<sup>†</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Beihang University, Beijing, China</span><br>
            <span class="eql-cntrb"><small><sup>*</sup>Equal Contribution. <sup>†</sup>Corresponding Author</small></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Arxiv PDF link -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <!-- Supplementary material PDF -->
              <!-- <span class="link-block">
                <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Supplementary</span>
                </a>
              </span> -->

              <!-- GitHub repository URL -->
              <span class="link-block">
                <a href="https://github.com/zeyuji/LLM-PeerReview" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>

              <!-- arXiv link -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/teaser.gif" alt="CoS framework" width="100%">
      <p>
        CoS accelerates collaborative decoding through two key enhancement: (1) refining verification mechanism of speculative decoding(SD) to extend SD to collaborative decoding scenarios. (2) incorporate an alternate proposal framework to further boosting inference speed.
      </p>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- <p>
            We propose <strong>LLM-PeerReview</strong>, an unsupervised LLM Ensemble method that selects the most ideal response from multiple LLM-generated candidates for each query, harnessing the collective wisdom of multiple models with diverse strengths.
          </p>
          <p>
            LLM-PeerReview is built on a novel, peer-review-inspired framework that offers a clear and interpretable mechanism, while remaining fully unsupervised for flexible adaptability and generalization.
          </p>
          <p>
            Specifically, LLM-PeerReview operates in three stages:
          </p>
          <ul>
            <li>For <em>scoring</em>, we use the emerging LLM-as-a-Judge technique to evaluate each response by reusing multiple LLMs at hand;</li>
            <li>For <em>reasoning</em>, we can apply a principled graphical model-based truth inference algorithm or a straightforward averaging strategy to aggregate multiple scores to produce a final score for each response;</li>
            <li>Finally, the highest-scoring response is selected as the ensemble output.</li>
          </ul>
          <p>
            LLM-PeerReview is conceptually simple and empirically powerful. 
            The two variants of the proposed approach obtain new state-of-the-art results across four datasets, including outperforming the recent advanced model Smoothie-Global by 6.9% and 7.3% points, respectively.
          </p> -->
          <p>
            We propose LLM-PeerReview, an unsupervised LLM Ensemble method that selects the most ideal response from multiple LLM-generated candidates for each query, harnessing the collective wisdom of multiple models with diverse strengths.
            LLM-PeerReview is built on a novel, peer-review-inspired framework that offers a clear and interpretable mechanism, while remaining fully unsupervised for flexible adaptability and generalization.
            Specifically, LLM-PeerReview operates in three stages:
            For <em>scoring</em>, we use the emerging LLM-as-a-Judge technique to evaluate each response by reusing multiple LLMs at hand;
            For <em>reasoning</em>, we can apply a principled graphical model-based truth inference algorithm or a straightforward averaging strategy to aggregate multiple scores to produce a final score for each response;
            Finally, the highest-scoring response is selected as the ensemble output.
            LLM-PeerReview is conceptually simple and empirically powerful. 
            The two variants of the proposed approach obtain new state-of-the-art results across four datasets, including outperforming the recent advanced model Smoothie-Global by 6.9% and 7.3% points, respectively.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Paper Motivation -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Motivation</h2>
      <p>
      <div class="content has-text-justified">
        <p>
          We propose Collaborative Decoding via Speculation (CoS), a novel framework that accelerates collaborative decoding, e.g. contrastive decoding or weighted ensemble, by leveraging speculative decoding principles. Its core innovations are:
        </p>

        <div class="columns">
          <div class="column">
            <div class="box">
              <h4 class="subtitle is-5">Verification Mechanism Refinement</h4>
              <p>
                Speculative decoding allows not only sampling from the target model's distribution, but also sampling from any combined distribution of the proposal model and target model.
              </p>
            </div>
          </div>
          
          <div class="column">
            <div class="box">
              <h4 class="subtitle is-5">Alternate Proposal Framework</h4>
              <p>
                In standard speculative decoding, the proposer and verifier are fixed, one model always acts as the proposer and the other model acts as the verifier, which is suboptimal in the collaborative decoding setting.
                </p><p> We observe that alternating each model as proposer and verifier can further speed up the collaboration process.
              </p>
            </div>
          </div>
        </div>
      </div>
      </p>
    </div>
  </div>
</section> -->

<!-- Overview of LLM-PeerReview -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container"> 
      <h2 class="title has-text-centered">Overview of LLM-PeerReview</h2>
      <img src="static/images/overview-LLM-PeerReview.png" alt="Overview of LLM-PeerReview" width="70%" class="center">
      
      <!-- <div class="content has-text-justified">
      <p>
        The proposed LLM-PeerReview contains three steps:
        <strong>(1) Scoring</strong>: 
        For a given query, after each LLM independently generates a response (analogous to a submitted academic paper), LLM-PeerReview applies the LLM-as-a-Judge technique, treating each model as a reviewer to assign scores to all candidate responses;
        <strong>(2) Reasoning</strong>: 
        LLM-PeerReview then uses a truth inference algorithm—analogous to a senior reviewer—to estimate a final score for each response.
        Notably, the graphical model-based inference algorithm is performed using score information across all queries, allowing the model to learn each LLM's scoring behavior using global information from the dataset, thereby enabling fine-grained, reliability-aware score aggregation;
        <strong>(3) Selecting the best:</strong>
        Finally, for each query, LLM-PeerReview selects the response with the highest final score as the ensemble output—analogous to how a senior reviewer chooses the best paper from a specific submission pool.
      </p>
      </div> -->

      <div class="content has-text-justified">
        <p>The proposed LLM-PeerReview contains three steps:</p>
        
        <ol>
          <li>
            <strong>Scoring</strong>: For a given query, after each LLM independently generates a response (analogous to a submitted academic paper), LLM-PeerReview applies the LLM-as-a-Judge technique, treating each model as a reviewer to assign scores to all candidate responses;
          </li>
          <li>
            <strong>Reasoning</strong>: LLM-PeerReview then uses a truth inference algorithm—analogous to a senior reviewer—to estimate a final score for each response. Notably, the graphical model-based inference algorithm is performed using score information across all queries, allowing the model to learn each LLM's scoring behavior using global information from the dataset, thereby enabling fine-grained, reliability-aware score aggregation;
          </li>
          <li>
            <strong>Selecting the best</strong>: Finally, for each query, LLM-PeerReview selects the response with the highest final score as the ensemble output—analogous to how a senior reviewer chooses the best paper from a specific submission pool.
          </li>
        </ol>
      </div>

      <!-- <iframe width="90%" height="675" class="center" style="margin-top: 1em;" src="https://www.youtube.com/embed/faLySXcU2DE?si=kLbdu0HECWLshR0t" title="YouTube video player" frameborder="0" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe> -->
    </div>
  </div>
</section>

<!-- <section class="section hero">
    <div class="container is-max-desktop">
      <div class="hero-body">
          <h1 class="title is-3 has-text-centered">Overview of LLM-PeerReview</h1>
          <img src="static/images/overview-LLM-PeerReview.png" alt="Symbolic-MoE" />
          <h2 class="subtitle has-text-left">
            The proposed LLM-PeerReview contains three steps:
            <strong>(1) Scoring</strong>: 
            For a given query, after each LLM independently generates a response (analogous to a submitted academic paper), LLM-PeerReview applies the LLM-as-a-Judge technique, treating each model as a reviewer to assign scores to all candidate responses;
            <strong>(2) Reasoning</strong>: 
            LLM-PeerReview then uses a truth inference algorithm—analogous to a senior reviewer—to estimate a final score for each response.
            Notably, the graphical model-based inference algorithm is performed using score information across all queries, allowing the model to learn each LLM's scoring behavior using global information from the dataset, thereby enabling fine-grained, reliability-aware score aggregation;
            <strong>(3) Selecting the best:</strong>
            Finally, for each query, LLM-PeerReview selects the response with the highest final score as the ensemble output—analogous to how a senior reviewer chooses the best paper from a specific submission pool.
          </h2>
      </div>
    </div>
  </section> -->


<!-- FRE Stage -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">CoS: Lossless Acceleration of Model Collaboration</h2>
      <p>
        The following video introduces the literature and most aspects of our approach. 
        To understand more technical details, please visit our <a href="https://kamichanw.github.io/publication/2025-02-01-cos">blog</a> or read the <a href="https://arxiv.org/pdf/2502.01662">paper</a>.
      </p>
      <iframe width="90%" height="675" class="center" style="margin-top: 1em;" src="https://www.youtube.com/embed/faLySXcU2DE?si=kLbdu0HECWLshR0t" title="YouTube video player" frameborder="0" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
    </div>
  </div>
</section> -->

<!-- Experiment Setup -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Experiment Setup</h2>
      
      <p><span class="paragraph-title">Datasets and evaluation.</span>
      We evaluate four widely-used datasets, grouped into three categories: (1) Factual Recall: TriviaQA evaluates the accuracy of model responses to factual questions across various domains; (2) Arithmetic Reasoning: GSM8k and MATH assess basic arithmetic and more advanced mathematical reasoning, respectively, with accuracy as the evaluation metric; (3) Instruction Following: AlpacaEval tests models' ability to follow various instructions, using GPT-4o-mini to evaluate accuracy. All experiments were performed using 6 or 4 parallel Nvidia V100 32GB GPUs, with stochastic outputs conducted three times.
      </p>
      
      <p><span class="paragraph-title">Seed LLMs and baselines.</span>
      Considering 7B-scale models are widely used and generally regarded as having acceptable judging capabilities, we use four well-established 7B models for ensemble: Llama-3.1-8B-Instruct, Mistral-7B-Instruct, Qwen2-7B-Instruct, and Qwen2.5-7B-Instruct. We compare the proposed LLM-PeerReview with two categories of baselines: (1) Single LLMs: the four 7B-scale models; (2) LLM Ensemble baselines: Random selection, Smoothie-Global, Smoothie-Local, and Agent-Forest.
      </p>
      
      <p><span class="paragraph-title">Configurations.</span>
      For each individual LLM, we follow the setup where the model responds once to each query. For our method, we set the model temperature to 0 during the scoring process to eliminate suboptimal results caused by randomness. The scoring prompts used across the four datasets are provided in the Appendix.
      </p>
    </div>
  </div>
</section> -->

<!-- Experiment Setup -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Experiment Setup</h2>
      
      <p><span class="paragraph-title">Datasets and evaluation.</span>
      We evaluate four widely-used datasets, grouped into three categories:
      <strong>(1) Factual Recall:</strong> TriviaQA evaluates the accuracy of model responses to factual questions across various domains, including history, science, and geography.
      <strong>(2) Arithmetic Reasoning:</strong> GSM8k and MATH assess basic arithmetic and more advanced mathematical reasoning, respectively, with accuracy as the evaluation metric, focusing on correct numerical answers.
      <strong>(3) Instruction Following:</strong> AlpacaEval tests models' ability to follow various instructions. We use GPT-4o-mini to evaluate the accuracy of model responses, assessing whether the model's response exceeds the reference answer in the dataset.
      </p>
      
      <p><span class="paragraph-title">Seed LLMs for ensemble.</span>
      Considering 7B-scale models are widely used by researchers and generally regarded as having acceptable judging capabilities, we use these well-established 7B models for ensemble: Llama-3.1-8B-Instruct, Mistral-7B-Instruct, Qwen2-7B-Instruct, and Qwen2.5-7B-Instruct.
      </p>
      
      <p><span class="paragraph-title">Baselines.</span>
      We compare the proposed LLM-PeerReview with two categories of baselines.
      <strong>(1) Single LLMs:</strong> The four 7B-scale models mentioned before.
      <strong>(2) LLM Ensemble baselines:</strong> 
      (<em>i</em>) Random is a random-selection baseline that simply returns the response from a randomly chosen LLM in the ensemble. As one of the simplest ensemble strategies for large language models, this method has previously been applied to dialogue tasks;
      (<em>ii</em>) Smoothie-Global, Smoothie-Local, and Agent-Forest are recently proposed, strong similarity-based ensemble methods.
      </p>
      
      <p><span class="paragraph-title">Configurations.</span>
      (1) For each individual large language model, we follow the setup of Smoothie, where the model responds once to each query. The responses from all models are stored for integration by the LLM Ensemble methods. (All LLM responses will also be open-sourced to promote reproducibility and further research.)
      (2) For the two variants of the baseline Smoothie, we set the number of neighbors as specified in the original paper. Agent-Forest does not require any hyperparameter configuration. For our method, we set the model temperature to 0 during the scoring process to eliminate suboptimal results caused by randomness.
      (3) All experiments were performed using 6 or 4 parallel Nvidia V100 32GB GPUs. All experiments with stochastic outputs were conducted three times.
      </p>
    </div>
  </div>
</section>

<!-- Results -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Results</h2>
      
      <div class="content has-text-justified">
      <p>From these tables and figures, we observe the following findings:</p>
      <!-- <ol style="padding-left: 3em;"> -->
      <!-- <ol>
        <li>
          <strong>The ensemble of the proposed LLM-PeerReview is effective.</strong> 
          As shown in Table 1, both of our variants consistently outperform any single LLM and all LLM Ensemble baselines across all datasets. 
          In the average performance, our two variant methods (67.4% and 67.8%) 
          surpass the strongest single model, Qwen2.5, by 4.7% and 5.1%, respectively, 
          and outperform the strongest ensemble method, Smoothie-Global, by 6.9% and 7.3%.
        </li>
        <li>
          <strong>Each LLM has its strengths and weaknesses.</strong> 
          Figure 3 shows that models with the best overall performance may underperform on specific tasks compared to those with weaker overall results. 
          This highlights that a strong LLM does not excel across all datasets, emphasizing the practical significance of LLM Ensemble.
        </li>
        <li>
          <strong>Aggregating multiple judges is crucial.</strong> 
          Our variants using single LLMs as judges perform well, but aggregating scores from multiple judges provides clear benefits. 
          Furthermore, the weighted truth inference in LLM-PeerReview-Weighted leads to additional performance gains compared to simple averaging, 
          as it effectively identifies stronger and weaker judges through learned transition matrices.
        </li>
        <li>
          <strong>The flipped-triple scoring trick represents a performance-efficiency trade-off.</strong> 
          As shown in Table 2, variants quadruple-half, flipped-triple, and double 
          all offer noticeable de-biasing performance advantages over the single-scoring strategy, 
          with flipped-triple achieving the best balance between performance and computational efficiency.
        </li>
        <li>
          <strong>Common scoring levels can generally be attempted.</strong> 
          Figure 5 shows that our method exhibits slightly varying performance across different scoring levels (3, 5, 7, and 10), 
          with no consistent tendencies, indicating robustness to scoring granularity.
        </li>
      </ol> -->

      <ol>
        <li>
          <strong>The ensemble of the proposed LLM-PeerReview is effective.</strong> 
          Both of our variants consistently outperform any single LLM and all LLM Ensemble baselines across all datasets. 
          In the average performance, our two variant methods (67.4% and 67.8%) 
          surpass the strongest single model, Qwen2.5, by 4.7% and 5.1%, respectively, 
          and outperform the strongest ensemble method, Smoothie-Global, by 6.9% and 7.3%.
        </li>
        <li>
          <strong>Each LLM has its strengths and weaknesses.</strong> 
          Our analysis reveals that models with the best overall performance may underperform on specific tasks compared to those with weaker overall results. 
          This highlights that a strong LLM does not excel across all datasets, emphasizing the practical significance of LLM Ensemble.
        </li>
        <li>
          <strong>Aggregating multiple judges is crucial.</strong> 
          Our variants using single LLMs as judges perform well, but aggregating scores from multiple judges provides clear benefits. 
          Furthermore, the weighted truth inference in LLM-PeerReview-Weighted leads to additional performance gains compared to simple averaging, 
          as it effectively identifies stronger and weaker judges through learned transition matrices.
        </li>
        <li>
          <strong>The flipped-triple scoring trick represents a performance-efficiency trade-off.</strong> 
          Our evaluation of different scoring strategies shows that variants quadruple-half, flipped-triple, and double 
          all offer noticeable de-biasing performance advantages over the single-scoring strategy, 
          with flipped-triple achieving the best balance between performance and computational efficiency.
        </li>
        <li>
          <strong>Common scoring levels can generally be attempted.</strong> 
          Our analysis demonstrates that the method exhibits slightly varying performance across different scoring levels, 
          with no consistent tendencies, indicating robustness to scoring granularity.
        </li>
      </ol>
      </div>

      <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
              <img src="static/images/table1.png" alt="Main experimental results" width="75%" class="center"/>
              <h2 class="subtitle has-text-centered">
                  <em><b>Main experimental results.</b></em> The method with the optimal performance is highlighted in <b>bold</b>.
              </h2>
          </div>
          <div class="item">
              <img src="static/images/fig3.png" alt="LLM performance analysis" width="30%" class="center"/>
              <h2 class="subtitle has-text-centered">
                  <em><b>LLM performance analysis.</b></em> Radar chart of individual LLM performance and win-tie-loss analysis demonstrating varying strengths across tasks.
              </h2>
          </div>
          <div class="item">
              <img src="static/images/fig4.png" alt="Transition matrices and correlation analysis" width="50%" class="center"/>
              <h2 class="subtitle has-text-centered">
                  <em><b>Transition matrices and correlation analysis.</b></em> Learned scoring behavior of each LLM and correlation with judging capability.
              </h2>
          </div>
          <div class="item">
              <img src="static/images/table2.png" alt="Scoring strategies comparison" width="80%" class="center"/>
              <h2 class="subtitle has-text-centered">
                  <em><b>Scoring strategies comparison.</b></em> Performance and computational efficiency of different scoring strategies.
              </h2>
          </div>
          <div class="item">
              <img src="static/images/fig5.png" alt="Scoring levels analysis" width="40%" class="center"/>
              <h2 class="subtitle has-text-centered">
                  <em><b>Scoring levels analysis.</b></em> Performance across different scoring granularities.
              </h2>
          </div>
      </div>
    </div>
  </div>
</section>

<!-- CoS Experiment Setup -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Experiment Setup</h2>
      <p><span class="paragraph-title">Dataset and evaluation.</span>
      We test CoS across multiple tasks including code generation, mathematical reasoning, multi-task understanding, and text summarization on HumanEval, GSM8K, MMLU, and CNNDM, respectively. We measure each method's speed by the average tokens generated per second and compute the speedup ratio relative to the standard collaborative decoding. All experiments are conducted on RTX 3090, except for evaluations involving the Llama-Vicuna model pair, which use the A6000 GPU.
      </p>
      <p><span class="paragraph-title">Combination functions and methods.</span>
        We experiment with two combination functions: weighted ensemble (WE) at the distribution level and contrastive decoding (CD) at the logits level.
        Among two combination functions, four methods are compared:  
        (1) the standard collaborative decoding (<strong>WE</strong>, <strong>CD</strong>);  
        (1) parallel collaborative decoding (<strong>WE-P</strong>, <strong>CD-P</strong>);  
        (2) an accelerated version with speculative decoding (SD), using the smallest model as the proposal and the combined distribution as the target (<strong>WE-SD</strong>, <strong>CD-SD</strong>);  
        and (3) CoS (<strong>WE-CoS</strong>, <strong>CD-CoS</strong>).
      </p>
      <p><span class="paragraph-title">Model pair configuration.</span>
        We experiment on different types and pairs of LLMs, as shown below.
      </p>
      <img src="static/images/model-pair.png" alt="Model pair configuration" width="40%" class="center">
    </div>
  </div>
</section> -->

<!-- CoS Results -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Results</h2>
      <p>From these tables, we observe the following findings:
      <ol style="padding-left: 3em;">
        <li>
          <strong>CoS consistently achieves the highest speedup</strong> across all settings, while SD may sometimes slow down collaboration. This is because vanilla SD doesn't guarantee acceleration—especially when the acceptance rate is low.
        </li>
        <li>
          <strong>CoS delivers a higher minimum speedup in the WE scenario</strong> compared to CD. In the two-model case, it reaches at least 1.34×, and in the three-model case, 1.27×. In contrast, CD can drop to 1.11×. This advantage comes from CoS maintaining a higher acceptance rate in WE.
        </li>
        <li>
          <strong>Speedup varies by task and output determinism.</strong> For example, in the WE scenario, CoS achieves 1.65× on HumanEval, where strict formatting favors high acceptance. On summarization tasks, with more flexible outputs, the speedup drops to 1.36×.
        </li>
      </ol></p>

      <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
              <img src="static/images/we-results.png" alt="The speedup ratio of each method in WE setting." width="30%" class="center"/>
              <h2 class="subtitle has-text-centered">
                  <em><b>The speedup ratio of each method in WE setting.</b></em> The method with the optimal speedup is highlighted in <b>bold</b>.
              </h2>
          </div>
          <div class="item">
              <img src="static/images/we-raw.png" alt="The raw speed of each method under WE setting." width="55%" class="center"/>
              <h2 class="subtitle has-text-centered">
                  <em><b>The raw speed of each method under WE setting.</b></em> The table reports the average number of tokens generated per second. Models are of comparable sizes.
              </h2>
          </div>
          <div class="item">
              <img src="static/images/cd-results.png" alt="The speedup ratio of each method in CD setting." width="40%" class="center"/>
              <h2 class="subtitle has-text-centered">
                  <em><b>The speedup ratio of each method in CD setting.</b></em>
              </h2>
          </div>
          <div class="item">
              <img src="static/images/cd-raw.png" alt="The raw speed of each method under CD setting." width="40%" class="center"/>
              <h2 class="subtitle has-text-centered">
                  <em><b>The raw speed of each method under CD setting.</b></em> Models are of different sizes.
              </h2>
          </div>
      </div>
  </div>
</section> -->


<!-- Paper poster -->
<section class="hero is-small is-light">
<div class="hero-body">
  <div class="container">
    <h2 class="title">Poster</h2>

    <!-- <iframe src="static/pdfs/our-paper.pdf" width="100%" height="550"> -->
    <iframe src="static/pdfs/Scoring, Reasoning, and Selecting the Best! Ensembling Large Language Models via a Peer-Review Process.pdf" width="100%" height="550">
    </iframe>
    
  </div>
</div>
</section>
<!--End paper poster -->


<!-- Add JavaScript for tab functionality -->
<!-- <script>
  document.addEventListener('DOMContentLoaded', function() {
    // Initialize tabs
    const tabs = document.querySelectorAll('#example-tabs li');
    const tabPanes = document.querySelectorAll('.tab-pane');
    
    console.log('Tabs initialized:', tabs.length, 'tabs found');
    
    // Debug: Print all tabs and their active state
    tabs.forEach(tab => {
      console.log('Tab:', tab.getAttribute('data-target'), 
                 'Active:', tab.classList.contains('is-active'));
    });
    
    // Debug: Print all panes and their active state
    tabPanes.forEach(pane => {
      console.log('Pane:', pane.id, 
                 'Active:', pane.classList.contains('is-active'),
                 'Display:', window.getComputedStyle(pane).display);
    });
    
    // Add click event to each tab
    tabs.forEach(tab => {
      tab.addEventListener('click', function() {
        // Get target tab pane ID
        const targetId = this.getAttribute('data-target');
        console.log('Tab clicked:', targetId);
        
        // Deactivate all tabs and tab panes
        document.querySelectorAll('#example-tabs li.is-active').forEach(t => t.classList.remove('is-active'));
        document.querySelectorAll('.tab-pane.is-active').forEach(p => p.classList.remove('is-active'));
        
        // Activate clicked tab and corresponding pane
        this.classList.add('is-active');
        const targetPane = document.getElementById(targetId);
        if (targetPane) {
          targetPane.classList.add('is-active');
          console.log('Activated pane:', targetId);
          
          // Re-render MathJax in the newly activated tab
          if (window.MathJax) {
            MathJax.typesetPromise([targetPane]).catch(err => console.error('MathJax error:', err));
          }
        } else {
          console.error('Target pane not found:', targetId);
        }
      });
    });
    
    // Load content from files
    loadExampleContent();
    
    // Ensure MathJax is properly initialized
    if (window.MathJax && typeof window.MathJax.typeset === 'function') {
      window.MathJax.startup = {
        ready: () => {
          console.log('MathJax is loaded and ready');
          MathJax.startup.defaultReady();
        }
      };
    }
  });
  
  // Function to load example content from files
  function loadExampleContent() {
    console.log('Loading example content from files');
    
    // Get all content containers
    const contentContainers = document.querySelectorAll('[data-file]');
    
    // Load content for each container
    contentContainers.forEach(container => {
      const filePath = container.getAttribute('data-file');
      console.log('Loading content from:', filePath);
      
      fetch(filePath)
        .then(response => {
          if (!response.ok) {
            throw new Error(`Failed to load file: ${filePath}`);
          }
          return response.text();
        })
        .then(text => {
          // Process text to properly format mathematical formulas
          // Replace LaTeX-style formulas with MathJax compatible format
          // Inline math: $formula$ -> \(formula\)
          // Display math: $$formula$$ -> \[formula\]
          const processedText = text
            .replace(/\$\$(.*?)\$\$/g, '\\[$1\\]')  // Display math
            .replace(/\$(.*?)\$/g, '\\($1\\)');     // Inline math
          
          // Set the content with HTML to preserve formatting
          container.innerHTML = processedText;
          
          // Typeset the math in this specific container
          if (window.MathJax) {
            MathJax.typesetPromise([container]).catch(err => console.error('MathJax error:', err));
          }
        })
        .catch(error => {
          console.error('Error loading file:', error);
          container.innerHTML = `<p class="has-text-danger">Error loading content: ${error.message}</p>`;
        });
    });
  }
</script> -->
  

<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
      <!-- <pre><code>@inproceedings{fu2025speculative,
      title={Fast Large Language Model Collaborative Decoding via Speculation},
      author={Fu, Jiale and Jiang, Yuchu and Chen, Junkai and Fan, Jiaming and Geng, Xin and Yang, Xu},
      booktitle={Forty-two International Conference on Machine Learning},
      year={2025}
    }</code></pre> -->
      <pre><code>TBD</code></pre>
  </div>
</section>
<!--End BibTex citation -->


<footer class="footer">
<div class="container">
  <div class="columns is-centered">
    <div class="column is-8">
      <div class="content">

        <p>
          This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
          You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
          Commons Attribution-ShareAlike 4.0 International License</a>.
        </p>

      </div>
    </div>
  </div>
</div>
</footer>

</body>
</html>
