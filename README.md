<p align="center">
  <h1 align="center">LLM-PeerReview</h1>



## 1 Introduction

We propose LLM-PeerReview, an unsupervised LLM Ensemble method that selects the most ideal response from multiple LLM-generated candidates for each query, harnessing the collective wisdom of multiple models with diverse strengths.

This version supports four datasets:

- **TriviaQA**: A dataset for question answering that covers a broad range of trivia questions.
- **GSM8K**: A dataset focused on mathematics question answering, with a special emphasis on problems requiring complex reasoning.
- **MATH**: A dataset containing challenging mathematical word problems that test advanced reasoning skills.
- **AlpacaEval**: A dataset designed for evaluating models in the context of conversational AI, with a focus on response quality and engagement.



## 2 Environment & Setup

### 2.1 System Requirements

Before using this project, please ensure your development environment meets the following requirements:

- **Operating System**: Linux (Ubuntu 20.04 or higher recommended)
- **Python Version**: Python 3.10.16 or higher
- **Hardware Requirements**:
  - **GPU**: NVIDIA V100 32GB or equivalent GPU for faster inference with large models (CUDA support required)
  - **Memory (RAM)**: Minimum 16GB RAM
  - **Storage**: At least 100GB of SSD storage for model and data processing
  - **Processor**: Intel i7 or higher, or equivalent AMD processor

### 2.2 Dependencies

To ensure the project runs smoothly, please install the required dependencies. Some of the key packages required for this project are:

```txt
torch==2.7.0
transformers==4.52.4
numpy==2.1.3
pandas==2.2.3
requests==2.32.3
scikit-learn==1.6.1
tensorflow==2.19.0
onnxruntime==1.22.0
huggingface-hub==0.32.3
pyyaml==6.0.2
```

You can install all the dependencies by running:

```bash
pip install -r requirements.txt
```

If you haven't downloaded the required large models yet, you can use the following script to download them:

```bash
bash ./Script/LLM_Download.sh
```

This will automatically download the necessary models and set them up in the appropriate directories.



## 3 Usage
### 3.1 Generate Response
To generate responses from the models, you can use the following script. This will run the model generation process and provide the output responses:

- **Generate Response using 7B Model**:
  
  ```bash
  bash ./Script/Response_Generate/New_7B_Response_Generate.sh
  ```
  
  This script will use the New 7B model to generate responses based on the input.

### 3.2 Model Scoring
To score the responses generated by different models, you can use the following script. This script uses our **PeerReview** method to score the responses:

- **Score Responses using PeerReview for GSM8K Dataset**:
  
  ```bash
  bash ./Script/Response_Scoring/judge/judge_gsm8k400.sh
  ```
  
  This script scores the responses generated by different models on the GSM8K dataset using our PeerReview method.

### 3.3 Ensemble Methods

To apply different ensemble methods for generating responses, you can use the following scripts. These methods combine the outputs from multiple models to generate a final response:

- **Random Ensemble Method**:
  ```bash
  bash ./Script/Ensemble_Generate/Random_Generate.sh
  ```
  
  This script generates responses by randomly selecting from the outputs of different models.
  
- **Peer Review Average Ensemble Method**:

  ```bash
  bash ./Script/Ensemble_Generate/PeerReview_Average_Generate.sh
  ```

  This script applies the Peer Review Average ensemble method, where multiple models' outputs are averaged to select the best response.

- **Peer Review Average with Truth Inference Ensemble Method**:

  ```bash
  bash ./Script/Ensemble_Generate/PeerReview_Average_Ti_Generate.sh
  ```

  This script applies the Peer Review Average method with Truth Inference (Ti), integrating multiple model responses based on a truth inference process.

### 3.4 Evaluation

Evaluation of the generated responses, scoring methods, and ensemble strategies is essential for comparing different approaches. The evaluation is divided into four main parts:

- **Evaluation of Model-Generated Responses**:
  
  ```bash
  bash ./Script/Response_Evaluate/New_7B_Response_Evaluate.sh
  ```
  This script evaluates the responses generated by the New 7B model based on predefined metrics.
  
- **Evaluation of Scored Responses (Model Scoring)**:

  ```bash
  bash ./Script/Response_Evaluate/New_7B_Judge_Response_Evaluate.sh
  ```

  This script evaluates the responses scored using the **PeerReview** method on the GSM8K dataset, assessing the quality of the responses based on the model's scoring.

- **Baseline Ensemble Evaluation**:

  ```bash
  bash ./Script/Response_Evaluate/Baseline_Ensemble_Response_Evaluate.sh
  ```

  This script evaluates the responses generated by the baseline ensemble method, which combines outputs from multiple models without any additional processing.

- **Evaluation of Our PeerReview Average Ensemble**:

  ```bash
  bash ./Script/Response_Evaluate/PeerReview_Average_Ensemble_Response_Evaluate.sh
  ```

  This script evaluates the responses generated using our **PeerReview Average Ensemble** method, where multiple models' responses are averaged to select the best answer.

